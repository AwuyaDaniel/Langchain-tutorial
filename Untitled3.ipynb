{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b4ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fcd216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\"temperature\": 0.6, \"max_length\": 64},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110cfc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mclaren'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLms\n",
    "llm(\"What is the fastest car in the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de5b0a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want to learn how to use deep learning to solve a problem'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"write me a youtube vidoe script about deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed7dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1870080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite me a youtube vidoe title about Deep Learning, just a title\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mDeep Learning - How to Use It\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite me a script about Deep Learning - How to Use It\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mDeep learning is a technique that can help you learn to recognize patterns in your data. It can also help you understand the patterns in your data and how to use them to make better decisions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Deep learning is a technique that can help you learn to recognize patterns in your data. It can also help you understand the patterns in your data and how to use them to make better decisions.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_template = PromptTemplate(input_variables = [\"topic\"], template = \"write me a youtube vidoe title about {topic}, just a title\")\n",
    "title_template_script = PromptTemplate(input_variables = [\"title\"], template = \"write me a script about {title}\")\n",
    "\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True)\n",
    "script_chain = LLMChain(llm=llm, prompt=title_template_script, verbose=True)\n",
    "\n",
    "sq_chain = SimpleSequentialChain(chains=[title_chain, script_chain], verbose=True)\n",
    "\n",
    "sq_chain.run(\"Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fad4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcf6a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite me a youtube vidoe title about Deep Learning, just a title\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite me a script about Deep Learning - How to Use It\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human: Deep Learning\\nAI: Deep Learning - How to Use It\\nHuman: Deep Learning\\nAI: Deep learning is a technique that can help you learn to recognize patterns in your data. It can also help you understand the patterns in your data and how to use them to make better decisions.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Template\n",
    "title_template = PromptTemplate(input_variables = [\"topic\"], template = \"write me a youtube vidoe title about {topic}, just a title\")\n",
    "title_template_script = PromptTemplate(input_variables = [\"title\"], template = \"write me a script about {title}\")\n",
    "\n",
    "#Memory\n",
    "memory = ConversationBufferMemory(input_key='topic', memory_key=\"chat_history\")\n",
    "\n",
    "\n",
    "# Llms\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key=\"title\", memory=memory)\n",
    "script_chain = LLMChain(llm=llm, prompt=title_template_script, verbose=True, output_key=\"script\", memory=memory)\n",
    "sq_chain = SequentialChain(chains=[title_chain, script_chain], verbose=True, input_variables=['topic'], output_variables=['title','script'])\n",
    "sq_chain({'topic':\"Deep Learning\"})\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5cae180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite me a youtube vidoe title about Deep Learning, just a title\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/awuyadaniel/anaconda3/envs/langchain/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite me a script about Deep Learning - How to Use It, while leveraging this wikipedia research: Page: Deep learning\n",
      "Summary: Deep learning is part of a broader family of machine learning methods, which is based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\n",
      "\n",
      "Page: Deep Learning (South Park)\n",
      "Summary: \"Deep Learning\" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall. Written and directed by Trey Parker, it premiered on March 8, 2023. The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Page: Deep learning\\nSummary: Deep learning is part of a broader family of machine learning methods, which is based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\\n\\nPage: Deep reinforcement learning\\nSummary: Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\\n\\nPage: Deep Learning (South Park)\\nSummary: \"Deep Learning\" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall. Written and directed by Trey Parker, it premiered on March 8, 2023. The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "\n",
    "\n",
    "# Template\n",
    "title_template = PromptTemplate(input_variables = [\"topic\"], template = \"write me a youtube vidoe title about {topic}, just a title\")\n",
    "title_template_script = PromptTemplate(input_variables = [\"title\", \"wikippedia_research\"], template = \"write me a script about {title}, while leveraging this wikipedia research: {wikippedia_research}\")\n",
    "\n",
    "#Memory\n",
    "title_memory = ConversationBufferMemory(input_key='topic', memory_key=\"chat_history\")\n",
    "script_memory = ConversationBufferMemory(input_key='title', memory_key=\"chat_history\")\n",
    "\n",
    "# Llms\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key=\"title\", memory=title_memory)\n",
    "script_chain = LLMChain(llm=llm, prompt=title_template_script, verbose=True, output_key=\"script\", memory=script_memory)\n",
    "# sq_chain = SequentialChain(chains=[title_chain, script_chain], verbose=True, input_variables=['topic'], output_variables=['title','script'])\n",
    "# sq_chain({'topic':\"Deep Learning\"})\n",
    "wiki = WikipediaAPIWrapper()\n",
    "title = title_chain.run(\"Deep Learning\")\n",
    "wiki_research = wiki.run(\"Deep Learning\")\n",
    "script = script_chain.run(title=title, wikippedia_research=wiki_research)\n",
    "\n",
    "title_memory.buffer\n",
    "script_memory.buffer\n",
    "wiki_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf8e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
